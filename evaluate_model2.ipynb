{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from src.image_tools import ImageTools\n",
    "from src.load_image import load_image_to_numpy_array\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEED TO SET::\n",
    "\n",
    "MODEL_NAME : Name of model used. For result csv etc.\n",
    "WORKING_DIR : if images are in a different folder than program\n",
    "GT : Ground-truth annotations (json format)\n",
    "PRED : Prediction output (json format)\n",
    "IMAGE_PATH: path to images from WORKING_DIR\n",
    "\n",
    "\"\"\"\n",
    "#WORKING_DIR = \"C:/Users/Sigurd/OneDriveMS/FYS-3741-MASTER/\"\n",
    "WORKING_DIR = \"C:/Users/sigur/OneDriveMS/FYS-3741-MASTER/\"\n",
    "GT = \"Labels/Annotations.json\"\n",
    "IMAGE_PATH = WORKING_DIR + \"data/data_yoloformat/test/images/\"\n",
    "#RESULT_NAME = \"_CONF_TESTING\"\n",
    "#GT = \"example_Annotations.json\"\n",
    "#IMAGE_PATH = 'examples/test_im/'\n",
    "\n",
    "#Faster RCNN 1\n",
    "#MODEL_NAME = \"Faster_RCNN_woDropout\"\n",
    "#MODEL_NAME = \"fasterrcnn_wDO_3ksteps\"\n",
    "#PRED = \"Labels/fasterrcnn_5ksteps_dropout5_result.json\"\n",
    "#PRED = \"Labels/result_faster_rcnn.json\"\n",
    "#RESULT_NAME = \"withdropout_confrange_0.1_0.9\"\n",
    "#RESULT_NAME = \"_IoUTesting\"\n",
    "#MODEL_NAME = \"Faster_RCNN_wdropout3ksteps\"\n",
    "\n",
    "#EfficientDet D1\n",
    "MODEL_NAME = \"EfficientDetD1\"\n",
    "PRED = \"Labels/result_efficientDetd1.json\"\n",
    "RESULT_NAME = \"_WITH_NMS_CONF01\"\n",
    "\n",
    "\n",
    "#YOLOv4\n",
    "#MODEL_NAME = \"YOLOv4\"\n",
    "#PRED = \"Labels/YOLOv4_results.json\"\n",
    "#RESULT_NAME = \"_conf_range_0.1_0.9\"\n",
    "#PRED = \"Labels/YOLOv4_iou0.5_thresh_0.1_results.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(json, conf_thresh = None):\n",
    "    df = pd.read_json(json)\n",
    "\n",
    "    if conf_thresh is not None:\n",
    "        pred = df[df['score'] >= conf_thresh]\n",
    "        return pred\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "class EvaluationTools:\n",
    "    def __init__(self, im_path, iou_threshold):\n",
    "\n",
    "        self.im_path = im_path\n",
    "        self.iou_threshold = iou_threshold\n",
    "    \n",
    "\n",
    "    def absolute_to_coordinates(self, bbox):\n",
    "        \"\"\"\n",
    "        convert bbox format from (xmin, ymin, w, h) to (xmin, ymin, xmax, ymax).\n",
    "        Param: bbox:\n",
    "                numpy array shape [n,4]\n",
    "        returns:\n",
    "                numpy array shape [n,4]\n",
    "        \"\"\"\n",
    "        converted_bbox = bbox.copy()\n",
    "        converted_bbox[:, 2] = bbox[:, 0] + bbox[:, 2]\n",
    "        converted_bbox[:, 3] = bbox[:, 1] + bbox[:, 3]\n",
    "\n",
    "        return converted_bbox\n",
    "\n",
    "\n",
    "    def get_image_info(self):\n",
    "        \"\"\"\n",
    "        Fetch image height and width\n",
    "        \"\"\"\n",
    "        im = load_image_to_numpy_array(self.im_path)\n",
    "        im_w = im.shape[1]\n",
    "        im_h = im.shape[0]\n",
    "        return im_w, im_h\n",
    "    \n",
    "    def get_iou(self, pred, gt):\n",
    "        \"\"\"\n",
    "        Calculate intersection over union (IoU) between all bounding boxes in image.\n",
    "        Also calculates total pixel area of each bounding box\n",
    "\n",
    "        param:\n",
    "            \n",
    "        returns:\n",
    "        iou: float(0,1)\n",
    "        area_bb1: float\n",
    "        area_bb2: float\n",
    "        \"\"\"\n",
    "        #remove values class/conf values and converte to coordinate form (pascalVOC)\n",
    "        bb1 = self.absolute_to_coordinates(pred)\n",
    "\n",
    "        #add extra axis for calculation\n",
    "        bb1 = bb1[:, None]\n",
    "        #remove class value and convert to coordinate form (pascalVOC)\n",
    "        bb2 = gt\n",
    "\n",
    "        #calculation...\n",
    "        low = np.s_[...,:2]\n",
    "        high = np.s_[..., 2:]\n",
    "\n",
    "        bb1[high] += 1; bb2[high] += 1\n",
    "        \n",
    "        #intersect\n",
    "        intrs = (np.maximum(0,np.minimum(bb1[high],bb2[high])\n",
    "                            -np.maximum(bb1[low],bb2[low]))).prod(-1)\n",
    "        #iou\n",
    "        area_pred = (bb1[high]-bb1[low]).prod(-1)\n",
    "        area_gt = (bb2[high]-bb2[low]).prod(-1)\n",
    "        iou = intrs / (area_pred + area_gt -intrs + 1e-16)\n",
    "\n",
    "        return iou, area_gt\n",
    "    \n",
    "    def find_valid_detections(self, iou, iou_threshold, index_list):\n",
    "        \"\"\"\n",
    "        find number TP, FP, FN \n",
    "        \"\"\"\n",
    "\n",
    "        closest_box = np.max(iou, axis=1)\n",
    "        closest_gt = np.max(iou, axis=0)\n",
    "\n",
    "        valid_boxes = closest_box.copy()\n",
    "        valid_boxes[np.where(closest_box <= iou_threshold)] = 0\n",
    "        valid_gt = closest_gt.copy()\n",
    "        valid_gt[np.where(closest_gt <= iou_threshold)] = 0\n",
    "\n",
    "\n",
    "        tp = np.zeros(len(index_list)); fp = np.zeros(len(index_list)); fn = np.zeros(len(index_list))\n",
    "\n",
    "        for i in range(len(index_list)):\n",
    "            try:\n",
    "                tp[i] += np.count_nonzero(valid_boxes[index_list[i]])\n",
    "                fp[i] += closest_box[index_list[i]].shape[0] - np.count_nonzero(valid_boxes[index_list[i]])\n",
    "            except:\n",
    "                tp[i] += np.count_nonzero(valid_gt[index_list[i]])\n",
    "                fn[i] = closest_gt[index_list[i]].shape[0] - np.count_nonzero(valid_gt[index_list[i]])\n",
    "                 \n",
    "        return tp, fp, fn\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def number_of_detections(self, area_gt):\n",
    "\n",
    "        area = np.ravel(area_gt)\n",
    "        small_thresh = 32**2\n",
    "        med_thresh = 96**2\n",
    "        \n",
    "        index_full = np.full(area.shape, True)\n",
    "        index_small = np.full(area.shape, False)\n",
    "        index_med = np.full(area.shape, False)\n",
    "        index_large = np.full(area.shape, False)\n",
    "        \n",
    "        for i in range(area.shape[0]):\n",
    "           \n",
    "            if area[i] < small_thresh:\n",
    "                index_small[i] = True\n",
    "\n",
    "            elif area[i] >= small_thresh and area[i] < med_thresh:\n",
    "                index_med[i] = True\n",
    "\n",
    "            elif area[i] >= med_thresh:\n",
    "                index_large[i] = True\n",
    "\n",
    "            else:\n",
    "                raise Exception('Value for bbox area is invalid')\n",
    "\n",
    "\n",
    "        return [index_full, index_small, index_med, index_large]\n",
    "\n",
    "    def no_predictions(self, gt_bbox):\n",
    "        \"\"\"\n",
    "        Handles situations where there are no valid predictions for a ground truth. Every gt object will be a false negative.\n",
    "        \"\"\"\n",
    "\n",
    "        low = np.s_[...,:2]\n",
    "        high = np.s_[..., 2:]\n",
    "\n",
    "        bb2 = gt_bbox\n",
    "        area_gt = (bb2[high]-bb2[low]).prod(-1)\n",
    "        indexes = self.number_of_detections(area_gt)\n",
    "\n",
    "        fn = np.sum(np.array([element*1 for element in indexes]), axis=1)\n",
    "        \n",
    "        return fn\n",
    "\n",
    "\n",
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn + 1e-10)\n",
    "\n",
    "\n",
    "def precision(tp, fp):\n",
    "    return tp / (tp + fp + 1e-10)\n",
    "\n",
    "\n",
    "def evaluate_results(MODEL_NAME, CONF_THRESH = 0.5, IOU_THRESH = 0.5):\n",
    "    TP = np.zeros(4)\n",
    "    FP = np.zeros(4)\n",
    "    FN = np.zeros(4)\n",
    "    PRED_LARGE = 0\n",
    "    PRED_MEDIUM = 0\n",
    "    PRED_SMALL = 0\n",
    "    PRED_TOTAL = 0\n",
    "\n",
    "    pred_df = json_to_df(json = PRED, conf_thresh=CONF_THRESH)\n",
    "    gt_df = json_to_df(GT)\n",
    "\n",
    "\n",
    "    for test_index in range(0, len(gt_df['image_id'].unique())):\n",
    "        #Get current image name\n",
    "        txt = IM[test_index]\n",
    "        #Get ID number from image name\n",
    "        file_number = re.findall(r'\\d+', txt)[0]\n",
    "        #Find all indexes matching current image for GT and pred\n",
    "        pred = pred_df[pred_df['image_id'] == txt]\n",
    "        gt = gt_df[gt_df['image_id'] == txt]\n",
    "\n",
    "        test = EvaluationTools(im_path=IMAGE_PATH+txt, iou_threshold=IOU_THRESH)\n",
    "\n",
    "        if len(pred) == 0:\n",
    "            gt_bbox = np.array([val for val in gt['bbox'].to_numpy()])\n",
    "            \n",
    "            tp = np.zeros(4)\n",
    "            fp = np.zeros(4)\n",
    "            fn = test.no_predictions(gt_bbox)\n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            FN += fn\n",
    "            continue\n",
    "        \n",
    "        #reshape array to remove annoying stuff from bad code\n",
    "        pred_bbox = np.array([val for val in pred['bbox'].to_numpy()]).round()\n",
    "        gt_bbox = np.array([val for val in gt['bbox'].to_numpy()])\n",
    "             \n",
    "\n",
    "        try:\n",
    "            iou, area_pred = test.get_iou(pred_bbox, gt_bbox)\n",
    "            area_indexes = test.number_of_detections(area_pred)\n",
    "            ########## CHHHHHHEEEEEEEEEEEEEECK TRANSPOSED\n",
    "            tp, fp, fn = test.find_valid_detections(iou, IOU_THRESH, area_indexes)\n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            FN += fn\n",
    "        \n",
    "        except:\n",
    "            print('Something is wrong with the TP/FP/FN calculation at index {}'.format(IM[test_index]))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #col_names = ['Conf_thresh: {}'.format(CONF_THRESH), 'Total', 'small:', 'medium:', 'large:']\n",
    "    results = {}\n",
    "\n",
    "    #col_names = ['{}, $\\tau = {}$'.format(MODEL_NAME, CONF_THRESH)]\n",
    "\n",
    "    keys = ['AP@',\n",
    "            'AR@',\n",
    "            'Precision Small',\n",
    "            'Precision Medium',\n",
    "            'Precision Large',\n",
    "            'Recall Small',\n",
    "            'Recall Medium',\n",
    "            'Recall Large']\n",
    "    \n",
    "    #AP/AR\n",
    "    values = [precision(TP[0], FP[0]), recall(TP[0], FN[0])]\n",
    "    #precision small/medium/large\n",
    "    for i in range(1, 4):\n",
    "        values.append(precision(TP[i], FP[i]))\n",
    "    #recall small/medium/large\n",
    "    for i in range(1, 4):\n",
    "        values.append(recall(TP[i], FN[i]))\n",
    "    \n",
    "    #save to dict\n",
    "    for i in range(len(keys)):\n",
    "        results[keys[i]] = values[i]\n",
    "    #print(results)\n",
    "\n",
    "    #df = pd.DataFrame.from_dict(results, orient = 'index')\n",
    "    return results, (TP, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Some old code\n",
    "\"\"\"\n",
    "IM = os.listdir(IMAGE_PATH)\n",
    "   \n",
    "    \n",
    "def main(MODEL_NAME):\n",
    "    output = pd.DataFrame()\n",
    "    col_names = []\n",
    "\n",
    "    # Run on single value\n",
    "    iou_thresh = 0.5\n",
    "    #conf_thresh = 0.1\n",
    "    #print(len(iou_thresh_range))\n",
    "    \n",
    "    #results = evaluate_results(MODEL_NAME=MODEL_NAME, CONF_THRESH=conf_thresh, IOU_THRESH=iou_thresh)\n",
    "    #output = pd.DataFrame([results]).T\n",
    "    #col_names.append(\"{}_iou_{}_conf{}\".format(MODEL_NAME, iou_thresh, conf_thresh))\n",
    "\n",
    "    ## For running many tests:\n",
    "\n",
    "    i = 0\n",
    "    #iou_thresh_range = range(10, 100, 5)\n",
    "    conf_thresh_range = range(10, 100, 10)\n",
    "\n",
    "    for conf in conf_thresh_range:\n",
    "        conf_thresh = conf / 100\n",
    "\n",
    "        sys.stdout.write('\\r Test: {}, completion: {}%'.format(i, np.round((i + 1) / (len(conf_thresh_range)) * 100, 2)))\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        results, conf_mat = evaluate_results(MODEL_NAME=MODEL_NAME, CONF_THRESH=conf_thresh, IOU_THRESH=iou_thresh)\n",
    "        df_dictionary = pd.DataFrame([results]).T\n",
    "        #print(df_dictionary)\n",
    "        output = pd.concat([output, df_dictionary], ignore_index=True, axis=1)\n",
    "        #print(output)\n",
    "        col_names.append(\"conf{}\".format(MODEL_NAME, conf_thresh))\n",
    "        #print(col_names)\n",
    "        i += 1\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    output.to_csv(r'results/{}_Evalutation_results.csv'.format(MODEL_NAME+RESULT_NAME), index = True, header=col_names)\n",
    "\n",
    "\n",
    "main(MODEL_NAME)\n",
    "\"\"\"\n",
    "#old code done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: [total, small, medium, large]\n",
      "[3136.    0. 1040. 2096.]\n",
      "False Positives: [total, small, medium, large]\n",
      "[275.   0. 119. 156.]\n",
      "False Negatives: [total, small, medium, large]\n",
      "[772.   4. 170. 598.]\n"
     ]
    }
   ],
   "source": [
    "IM = os.listdir(IMAGE_PATH)\n",
    "\n",
    "\n",
    "conf_df = pd.DataFrame()\n",
    "output = pd.DataFrame()\n",
    "col_names = []\n",
    "conf_thresh = 0.1\n",
    "\n",
    "# Run on single value\n",
    "iou_thresh = 0.5\n",
    "\n",
    "#print(len(iou_thresh_range))\n",
    "\n",
    "results, conf_mat = evaluate_results(MODEL_NAME=MODEL_NAME, CONF_THRESH=conf_thresh, IOU_THRESH=iou_thresh)\n",
    "output = pd.DataFrame([results]).T\n",
    "col_names.append(\"{}_iou_{}_conf{}\".format(MODEL_NAME, iou_thresh, conf_thresh))\n",
    "\n",
    "## For running many tests:\n",
    "\"\"\"\n",
    "i = 0\n",
    "iou_thresh_range = range(10, 100, 5)\n",
    "#conf_thresh_range = range(10, 100, 10)\n",
    "\n",
    "for iou in iou_thresh_range:\n",
    "    #conf_thresh = conf / 100\n",
    "    iou_thresh = iou / 100\n",
    "    sys.stdout.write('\\r Test: {}, completion: {}%'.format(i, np.round((i + 1) / (len(iou_thresh_range)) * 100, 2)))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    results, conf_mat = evaluate_results(MODEL_NAME=MODEL_NAME, CONF_THRESH=conf_thresh, IOU_THRESH=iou_thresh)\n",
    "    df_dictionary = pd.DataFrame([results]).T\n",
    "    #print(df_dictionary)\n",
    "    conf = [conf_mat[i][0] for i in range(3)]\n",
    "    output_conf_mat = pd.DataFrame(conf, columns = [iou])\n",
    "    conf_df = pd.concat([conf_df, output_conf_mat], ignore_index=True, axis=1)\n",
    "\n",
    "\n",
    "    output = pd.concat([output, df_dictionary], ignore_index=True, axis=1)\n",
    "    #print(output)\n",
    "    col_names.append(\"conf{}\".format(MODEL_NAME, conf_thresh))\n",
    "    #print(col_names)\n",
    "    i += 1\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\"\"\"\n",
    "#output.to_csv(r'results/{}_Evalutation_results.csv'.format(MODEL_NAME+RESULT_NAME), index = True, header=col_names)\n",
    "#conf_df.to_csv(r'results/{}_Evalutation_results.csv'.format(MODEL_NAME+RESULT_NAME+'Conf_mat'), index = True, header=col_names)\n",
    "#print(conf_mat_df)\n",
    "\n",
    "TP = conf_mat[0]; FP = conf_mat[1]; FN = conf_mat[2]\n",
    "\n",
    "\n",
    "print('True Positives: [total, small, medium, large]')\n",
    "print(TP)\n",
    "print('False Positives: [total, small, medium, large]')\n",
    "print(FP)\n",
    "print('False Negatives: [total, small, medium, large]')\n",
    "print(FN)\n",
    "\n",
    "\n",
    "#main(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: [total, small, medium, large]\n",
      "[3136.    0. 1040. 2096.]\n",
      "False Positives: [total, small, medium, large]\n",
      "[275.   0. 119. 156.]\n",
      "False Negatives: [total, small, medium, large]\n",
      "[772.   4. 170. 598.]\n",
      "recall: \n",
      "[0.8024565  0.         0.85950413 0.77802524]\n",
      "precision:  [0.91937848 0.         0.89732528 0.93072824]\n"
     ]
    }
   ],
   "source": [
    "###PLOT\n",
    "\n",
    "print('True Positives: [total, small, medium, large]')\n",
    "print(TP)\n",
    "print('False Positives: [total, small, medium, large]')\n",
    "print(FP)\n",
    "print('False Negatives: [total, small, medium, large]')\n",
    "print(FN)\n",
    "\n",
    "\n",
    "r = recall(TP, FN)\n",
    "p = precision(TP, FP)\n",
    "print('recall: ')\n",
    "print(r)\n",
    "print('precision: ', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4496553479793356\n",
      "0.6939601151422186\n"
     ]
    }
   ],
   "source": [
    "#mAP = np.mean(ap[8:])\n",
    "#mAR = np.mean(ar[8:])\n",
    "#print(mAP)\n",
    "#print(mAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([12.,  0.,  4.,  8.]), array([3734.,    0., 1073., 2661.]), array([437.,   4., 252., 181.]))\n"
     ]
    }
   ],
   "source": [
    "#print(conf_mat)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13628a9972f3db4bb1d84f92e59ef7b5ba6082c55cb4a92f18f05ec8a7c2f0c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
